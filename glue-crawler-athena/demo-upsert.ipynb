{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"## Simple UPSERT Delta Lake\n",
				"\n",
				"**Prerequisites**:\n",
				" \n",
				"- Download data [here](https://aws.amazon.com/blogs/big-data/handle-upsert-data-operations-using-open-source-delta-lake-and-aws-glue/)\n",
				"- Crawl the data \n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Enable Delta Lake Support "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%glue_version 3.0\n",
				"%%configure\n",
				"{\n",
				"  \"--datalake-formats\": \"delta\"\n",
				"}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%help"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%idle_timeout 2880\n",
				"# %glue_version 3.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 5\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Read Fulload Delta Table Using SQL "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%%sql\n",
				"SELECT * FROM `default`.`sample_delta_sample_delta_table` limit 10"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"## Upsert Delta Table using Spark DataFrame"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"BUCKET=\"upsert-demo-15092023\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"from delta.tables import DeltaTable\n",
				"from pyspark.sql.types import *\n",
				"from pyspark.sql.functions import col\n",
				"from pyspark.sql.functions import expr"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"schema = StructType() \\\n",
				"      .add(\"policy_id\",IntegerType(),True) \\\n",
				"      .add(\"expiry_date\",DateType(),True) \\\n",
				"      .add(\"location_name\",StringType(),True) \\\n",
				"      .add(\"state_code\",StringType(),True) \\\n",
				"      .add(\"region_name\",StringType(),True) \\\n",
				"      .add(\"insured_value\",IntegerType(),True) \\\n",
				"      .add(\"business_type\",StringType(),True) \\\n",
				"      .add(\"earthquake_coverage\",StringType(),True) \\\n",
				"      .add(\"flood_coverage\",StringType(),True) "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# read the full load\n",
				"sdf = spark.read.format(\"csv\").option(\"header\",True).schema(schema).load(f's3://{BUCKET}/fullload/')\n",
				"sdf.printSchema()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# sdf.show(5)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# write data as DELTA TABLE\n",
				"sdf.write.format(\"delta\").mode(\"overwrite\").save(\"s3://\"+ BUCKET +\"/delta/insurance/\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# please crawl it before can query \n",
				"# %%sql \n",
				"# SELECT * FROM `default`.`delta_insurance`"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# read cdc update data \n",
				"cdc_df = spark.read.csv(f's3://{BUCKET}/cdcload/')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# df_update.show(5)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# read fullload to dataframe from existing delta table \n",
				"delta_df = DeltaTable.forPath(spark, \"s3://\"+ BUCKET +\"/delta/insurance/\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# delta_df.toDF().show(5,True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# UPSERT process if matches on the condition the update else insert\n",
				"# if there is no keyword then create a data set with Insert, Update and Delete flag and do it separately.\n",
				"# for delete it has to run in loop with delete condition, this script do not handle deletes.\n",
				"    \n",
				"final_df = delta_df.alias(\"prev_df\").merge( \\\n",
				"source = cdc_df.alias(\"append_df\"), \\\n",
				"#matching on primarykey\n",
				"condition = expr(\"prev_df.policy_id = append_df._c1\"))\\\n",
				".whenMatchedUpdate(set= {\n",
				"    \"prev_df.expiry_date\"           : col(\"append_df._c2\"), \n",
				"    \"prev_df.location_name\"         : col(\"append_df._c3\"),\n",
				"    \"prev_df.state_code\"            : col(\"append_df._c4\"),\n",
				"    \"prev_df.region_name\"           : col(\"append_df._c5\"), \n",
				"    \"prev_df.insured_value\"         : col(\"append_df._c6\"),\n",
				"    \"prev_df.business_type\"         : col(\"append_df._c7\"),\n",
				"    \"prev_df.earthquake_coverage\"   : col(\"append_df._c8\"), \n",
				"    \"prev_df.flood_coverage\"        : col(\"append_df._c9\")} )\\\n",
				".whenNotMatchedInsert(values =\n",
				"#inserting a new row to Delta table\n",
				"{   \"prev_df.policy_id\"             : col(\"append_df._c1\"),\n",
				"    \"prev_df.expiry_date\"           : col(\"append_df._c2\"), \n",
				"    \"prev_df.location_name\"         : col(\"append_df._c3\"),\n",
				"    \"prev_df.state_code\"            : col(\"append_df._c4\"),\n",
				"    \"prev_df.region_name\"           : col(\"append_df._c5\"), \n",
				"    \"prev_df.insured_value\"         : col(\"append_df._c6\"),\n",
				"    \"prev_df.business_type\"         : col(\"append_df._c7\"),\n",
				"    \"prev_df.earthquake_coverage\"   : col(\"append_df._c8\"), \n",
				"    \"prev_df.flood_coverage\"        : col(\"append_df._c9\")\n",
				"})\\\n",
				".execute()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"# read target table \n",
				"delta_df = DeltaTable.forPath(spark, \"s3://\"+ BUCKET +\"/delta/insurance/\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"delta_df.toDF().select(\"policy_id\", \"expiry_date\").show(3)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Filter on DataFrame \n",
				"\n",
				"Both filter and where work but where is for SQL familiarity"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"temp_df = delta_df.toDF()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"temp_df.where(\"policy_id IN (100462, 100463, 100475)\").show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Temp View Table and Query "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"temp_df.createOrReplaceTempView(\"temp_view\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"spark.sql(\"SELECT * FROM temp_view WHERE policy_id IN (100462, 100463, 100475)\").show()"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"pygments_lexer": "python3",
			"version": "3.9.17"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
